{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1553,
     "status": "ok",
     "timestamp": 1668608912509,
     "user": {
      "displayName": "yq huang",
      "userId": "12028814288211570124"
     },
     "user_tz": -480
    },
    "id": "D29L3vvzv738"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zffMEk57UR2"
   },
   "source": [
    "## 1. Introduction to the Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, the order of the implementation of the problem is as follows:\n",
    "1. We check for the meaning of each feature\n",
    "2. We check for the possible existence of missing values and duplicate terms\n",
    "3. We check for the unexpected/unexplained values in some variables\n",
    "4. We check for the distribution of each variable, including both categorical and continuous\n",
    "5. We check whether the data is unbalanced\n",
    "6. We do the data preprocessing, including category consolidation (replace the unexplained values with reasonable existing value), data encoding and scaling\n",
    "7. We split the train and test set\n",
    "8. We perform feature selection (correlation test, chi-squared test, SelectKBest method, backward feature selection) on the train set\n",
    "9. We drop the unnecessary features from both train and test set\n",
    "10. We perform the linear separability check on the train set\n",
    "11. We apply Synthetic Minority Over-sampling Technique (SMOTE) to train set, and export the train and test set to separate csv files\n",
    "12. We use different models to fit the train set, then apply the model to the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 540,
     "status": "ok",
     "timestamp": 1668608932485,
     "user": {
      "displayName": "yq huang",
      "userId": "12028814288211570124"
     },
     "user_tz": -480
    },
    "id": "u3ZhPirWvtCv"
   },
   "outputs": [],
   "source": [
    "# card_divided uses prof's code to split the data file into train and test set\n",
    "# the first 22500 rows makes up the train set, the remaining rows are test set\n",
    "df_card = pd.read_csv('card_divided.csv',skiprows=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1668608932486,
     "user": {
      "displayName": "yq huang",
      "userId": "12028814288211570124"
     },
     "user_tz": -480
    },
    "id": "lMtWYie87S5x",
    "outputId": "94714c3c-349f-4449-8300-7613a184ef80"
   },
   "outputs": [],
   "source": [
    "df_card.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1668608932486,
     "user": {
      "displayName": "yq huang",
      "userId": "12028814288211570124"
     },
     "user_tz": -480
    },
    "id": "Znbtxd6txV9O",
    "outputId": "194e6b0a-d42b-4197-f3ed-2b5c183467e1"
   },
   "outputs": [],
   "source": [
    "df_card.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see summary statistics\n",
    "df_card.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "executionInfo": {
     "elapsed": 2047,
     "status": "ok",
     "timestamp": 1668608934529,
     "user": {
      "displayName": "yq huang",
      "userId": "12028814288211570124"
     },
     "user_tz": -480
    },
    "id": "ceBESLbXC7zk",
    "outputId": "fa9f9069-58ea-4b09-d025-10380e97f8ac"
   },
   "outputs": [],
   "source": [
    "# Duplicate and missing data\n",
    "\n",
    "# Check for missing values\n",
    "# get the number of missing records in each column\n",
    "print(df_card.isnull().sum())\n",
    "print(\"Presence of missing values: \",  df_card.isnull().values.any())\n",
    "\n",
    "# Check for duplicates - use ID since each client will have a different ID\n",
    "print(\"Number of unique IDs : \" , df_card['ID'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary statistics, we can see that `AGE` has a minimum of 21 and maximum of 79. Since the minimum age to have a credit card in Taiwan is 18 (Pinzon, 2023), all the data is valid.\n",
    "\n",
    "Reference: \n",
    "Pinzon, J. L. (2023, January 5). Young Taiwanese adults can open bank accounts themselves│TVBS新聞網. TVBS. Retrieved April 11, 2023, from https://news.tvbs.com.tw/english/2009577"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the summary table, we can see that the PAY_n values have a minimum of -2, which was not defined\n",
    "# so we check here to see if it's the same for all the PAY_n values\n",
    "print(df_card.PAY_0.min())\n",
    "print(df_card.PAY_2.min())\n",
    "print(df_card.PAY_3.min())\n",
    "print(df_card.PAY_4.min())\n",
    "print(df_card.PAY_5.min())\n",
    "print(df_card.PAY_6.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of categorical variables\n",
    "print(df_card.SEX.value_counts())\n",
    "print('Sex column values: ', df_card.SEX.unique())\n",
    "print(df_card.EDUCATION.value_counts())\n",
    "print('Education column values: ', df_card.EDUCATION.unique())\n",
    "print(df_card.MARRIAGE.value_counts())\n",
    "print('Marriage column values: ', df_card.MARRIAGE.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise distribution of categorical variables\n",
    "sex_counts = df_card.SEX.value_counts()\n",
    "education_counts = df_card.EDUCATION.value_counts()\n",
    "marriage_counts = df_card.MARRIAGE.value_counts()\n",
    "\n",
    "# sex pie chart\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(sex_counts, labels=sex_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "ax.legend(labels=['1 = Male', '2 = Female'], loc='upper right')\n",
    "ax.set_title('Distribution of SEX')\n",
    "ax.axis('equal') # to draw a circle\n",
    "\n",
    "# education pie chart\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(education_counts, labels=education_counts.index, startangle=90)\n",
    "ax1.legend(labels=['1 = graduate school', '2 = university', '3 = high school',\n",
    "                     '4 = others', '5 = unknown', '6 = unknown'], loc='upper right')\n",
    "ax1.set_title('Distribution of EDUCATION')\n",
    "ax1.axis('equal')\n",
    "\n",
    "# marriage pie chart\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.pie(marriage_counts, labels=marriage_counts.index, startangle=90)\n",
    "ax2.legend(labels=['1 = married', '2 = single', '3 = others'], loc='lower right')\n",
    "ax2.set_title('Distribution of MARRIAGE')\n",
    "ax2.axis('equal')\n",
    "\n",
    "# show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "executionInfo": {
     "elapsed": 1816,
     "status": "ok",
     "timestamp": 1668608936342,
     "user": {
      "displayName": "yq huang",
      "userId": "12028814288211570124"
     },
     "user_tz": -480
    },
    "id": "AsQZMxDVF2qz",
    "outputId": "e7e39f8a-9c80-4468-988e-2087b18d4035"
   },
   "outputs": [],
   "source": [
    "# Distribution of Continuous Variable\n",
    "def hist(df, vars, n_r, n_c, n_b):\n",
    "    fig=plt.figure()\n",
    "    for i, variable_name in enumerate(vars):\n",
    "        ax=fig.add_subplot(n_r,n_c,i+1)\n",
    "        df[variable_name].hist(bins=n_b,ax=ax)\n",
    "        ax.set_title(variable_name, fontsize = 8)\n",
    "    fig.tight_layout() \n",
    "    plt.show()\n",
    "\n",
    "continous_variables = df_card[['LIMIT_BAL','BILL_AMT1','BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
    "                 'PAY_AMT1','PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']]\n",
    "hist(continous_variables, continous_variables.columns, 4, 4, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1668608938962,
     "user": {
      "displayName": "yq huang",
      "userId": "12028814288211570124"
     },
     "user_tz": -480
    },
    "id": "BDfIhShq8vL9",
    "outputId": "68a8fdde-82d7-4497-e931-3d15f0e1766e"
   },
   "outputs": [],
   "source": [
    "# check financial stats of clients with high LIMIT_BAL to check for outliers\n",
    "# sort from highest\n",
    "df_sorted = df_card.sort_values(by='LIMIT_BAL', ascending=False)\n",
    "# top 20 highest LIMIT_BAL\n",
    "top_20 = df_sorted.head(20)\n",
    "top_20_stats = top_20.loc[:, ['LIMIT_BAL', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'BILL_AMT2', 'BILL_AMT3', \n",
    "                              'BILL_AMT4', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'default.payment.next.month']]\n",
    "print(top_20_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of target variable\n",
    "print(df_card['default.payment.next.month'].value_counts())\n",
    "\n",
    "# pie chart\n",
    "target_counts = df_card['default.payment.next.month'].value_counts()\n",
    "fig3, ax3 = plt.subplots()\n",
    "ax3.pie(target_counts, labels=target_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "ax3.legend(labels=['0 = non-default', '1 = default'], loc='lower right')\n",
    "ax3.set_title('Distribution of target variable')\n",
    "ax3.axis('equal')\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace value 0 with the category \"others\" represented by 3\n",
    "df_card.loc[df_card.MARRIAGE == 0, 'MARRIAGE'] = 3\n",
    "\n",
    "# replace values 0, 5, 6 with the category \"others\" represented by 4\n",
    "education_others = (df_card.EDUCATION == 5) | (df_card.EDUCATION == 6) | (df_card.EDUCATION == 0)\n",
    "df_card.loc[education_others, 'EDUCATION'] = 4\n",
    "\n",
    "# replace all PAY_n values -2, -1 with 0\n",
    "paid_duly = (df_card.PAY_0 == -2) | (df_card.PAY_0 == -1)\n",
    "df_card.loc[paid_duly, 'PAY_0'] = 0\n",
    "\n",
    "paid_duly = (df_card.PAY_2 == -2) | (df_card.PAY_2 == -1)\n",
    "df_card.loc[paid_duly, 'PAY_2'] = 0\n",
    "\n",
    "paid_duly = (df_card.PAY_3 == -2) | (df_card.PAY_3 == -1)\n",
    "df_card.loc[paid_duly, 'PAY_3'] = 0\n",
    "\n",
    "paid_duly = (df_card.PAY_4 == -2) | (df_card.PAY_4 == -1)\n",
    "df_card.loc[paid_duly, 'PAY_4'] = 0\n",
    "\n",
    "paid_duly = (df_card.PAY_5 == -2) | (df_card.PAY_5 == -1)\n",
    "df_card.loc[paid_duly, 'PAY_5'] = 0\n",
    "\n",
    "paid_duly = (df_card.PAY_6 == -2) | (df_card.PAY_6 == -1)\n",
    "df_card.loc[paid_duly, 'PAY_6'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Encoding\n",
    "df_card_encoded = df_card.copy()\n",
    "\n",
    "# one-hot encoding - SEX\n",
    "# drop_first = True drops the first category of SEX (Male)\n",
    "df_card_encoded = pd.get_dummies(df_card_encoded, columns = ['SEX'], drop_first = True)\n",
    "# rename column to FEMALE\n",
    "df_card_encoded.rename(columns={'SEX_2':'FEMALE'}, inplace=True)\n",
    "\n",
    "# one-hot encoding - MARRIAGE\n",
    "df_card_encoded = pd.get_dummies(df_card_encoded, columns = ['MARRIAGE'])\n",
    "# drop MARRIAGE_3 (Others category) column\n",
    "df_card_encoded = df_card_encoded.drop('MARRIAGE_3',axis = 1)\n",
    "# rename columns\n",
    "df_card_encoded.rename(columns={'MARRIAGE_1':'MARRIED', 'MARRIAGE_2': 'SINGLE'}, inplace=True)\n",
    "\n",
    "print(df_card_encoded.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df_card_encoded_scaled = df_card_encoded.copy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_card_encoded_scaled[['LIMIT_BAL','BILL_AMT1','BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
    "                        'PAY_AMT1','PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']] = pd.DataFrame(\n",
    "    scaler.fit_transform(df_card_encoded_scaled[['LIMIT_BAL','BILL_AMT1','BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', \n",
    "                                                 'BILL_AMT5', 'BILL_AMT6','PAY_AMT1','PAY_AMT2', 'PAY_AMT3', \n",
    "                                                 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']]))\n",
    "print(df_card_encoded_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test and train set\n",
    "X_train = df_card_encoded_scaled.iloc[0:22500,1:]\n",
    "X_test = df_card_encoded_scaled.iloc[22500:30000,1:]\n",
    "y_train = df_card_encoded_scaled.iloc[0:22500,-4]\n",
    "y_test = df_card_encoded_scaled.iloc[22500:30000,-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "# X is the 23 features, y is the target variable\n",
    " \n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "corr= X_train.corr()\n",
    "plt.figure(figsize=(18,15))\n",
    "mask = np.triu(np.ones_like(corr))\n",
    "sns.heatmap(corr, annot=True, mask = mask, vmin=-1.0, vmax=1, cmap ='vlag')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1668608940382,
     "user": {
      "displayName": "yq huang",
      "userId": "12028814288211570124"
     },
     "user_tz": -480
    },
    "id": "Msx_yiPQK1M3",
    "outputId": "1adbdb17-3421-426f-cb22-09bc50ef8f1d"
   },
   "outputs": [],
   "source": [
    "# Chi2\n",
    "\n",
    "# data without target column\n",
    "X_train_only_features = X_train.drop(columns=['default.payment.next.month'],axis = 1)\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "chi_scores = chi2(np.clip(X_train_only_features,0,10000000),y)\n",
    "print(chi_scores)\n",
    "p_values = pd.Series(chi_scores[1],index = X_train_only_features.columns) \n",
    "print(p_values)\n",
    "p_values.sort_values(ascending = False , inplace = True) \n",
    "p_values.plot.bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Chi-squared test, all bill amounts can be removed. However, bill amount can be useful in determining the target variable. Generally, those with higher outstanding debt in their bill amount have a higher risk of default (Kagan, 2023) since they may struggle to make larger payments.\n",
    "\n",
    "Reference: Kagan, J. (2023, April 6). Default risk: Definition, types, and ways to measure. Investopedia. Retrieved April 13, 2023, from https://www.investopedia.com/terms/d/defaultrisk.asp  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelectKBest\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# for classification, we use these three\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
    "\n",
    "# this function will take in X, y variables \n",
    "# with criteria, and return a dataframe with most important columns based on that criteria\n",
    "def featureSelect_dataframe(X, y, criteria, k):\n",
    "\n",
    "    # initialize our function/method\n",
    "    reg = SelectKBest(criteria, k=k).fit(X,y)\n",
    "    \n",
    "    # transform after creating the reg (so we can use getsupport)\n",
    "    X_transformed = reg.transform(X)\n",
    "\n",
    "    # filter down X based on kept columns\n",
    "    X = X[[val for i,val in enumerate(X.columns) if reg.get_support()[i]]]\n",
    "\n",
    "    # return that dataframe\n",
    "    return X\n",
    "\n",
    "New_X = featureSelect_dataframe(X_train_only_features, y_train, chi2, 11)\n",
    "\n",
    "New_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward feature selection\n",
    "X_train = X_train_only_features\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "import pandas as pd\n",
    "\n",
    "# Instantiate the logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Instantiate the Recursive Feature Elimination (RFE) object\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "\n",
    "# Fit the RFE to the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Create a dataframe to store the feature rankings\n",
    "feature_ranks = pd.DataFrame({'Feature': X_train.columns, 'Rank': rfe.ranking_})\n",
    "\n",
    "# Sort the features by rank\n",
    "feature_ranks = feature_ranks.sort_values(by='Rank')\n",
    "\n",
    "# Print out the feature ranking with each variable name\n",
    "for rank, feature in enumerate(feature_ranks['Feature']):\n",
    "    print(f\"Rank {rank+1}: {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Drop columns we are not using \n",
    "df_card_encoded_scaled = df_card_encoded_scaled.drop('BILL_AMT1',axis = 1)\n",
    "df_card_encoded_scaled = df_card_encoded_scaled.drop('BILL_AMT3',axis = 1)\n",
    "df_card_encoded_scaled = df_card_encoded_scaled.drop('BILL_AMT4',axis = 1)\n",
    "df_card_encoded_scaled = df_card_encoded_scaled.drop('BILL_AMT5',axis = 1)\n",
    "df_card_encoded_scaled = df_card_encoded_scaled.drop('PAY_2',axis = 1)\n",
    "df_card_encoded_scaled = df_card_encoded_scaled.drop('PAY_3',axis = 1)\n",
    "df_card_encoded_scaled = df_card_encoded_scaled.drop('PAY_5',axis = 1)\n",
    "df_card_encoded_scaled = df_card_encoded_scaled.drop('AGE',axis = 1)\n",
    "\n",
    "print(df_card_encoded_scaled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3288,
     "status": "ok",
     "timestamp": 1668608947930,
     "user": {
      "displayName": "yq huang",
      "userId": "12028814288211570124"
     },
     "user_tz": -480
    },
    "id": "HrngtfSgTOqA",
    "outputId": "e84e85ec-e2e2-4a76-a5e2-da2798fb163d"
   },
   "outputs": [],
   "source": [
    "# Split test and train set again after dropping the features, this time the dataset is without redundant features\n",
    "X_train = df_card_encoded_scaled.iloc[0:22500,1:]\n",
    "X_test = df_card_encoded_scaled.iloc[22500:30000,1:]\n",
    "y_train = df_card_encoded_scaled.iloc[0:22500,-4]\n",
    "y_test = df_card_encoded_scaled.iloc[22500:30000,-4]\n",
    "\n",
    "del X_train['default.payment.next.month']\n",
    "del X_test['default.payment.next.month']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Linear Separability Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 953264,
     "status": "ok",
     "timestamp": 1668590999904,
     "user": {
      "displayName": "yq huang",
      "userId": "12028814288211570124"
     },
     "user_tz": -480
    },
    "id": "nCsiHSqyhkxN",
    "outputId": "89f17097-e28e-402e-c8c9-e929132541bf"
   },
   "outputs": [],
   "source": [
    "# linear separability check using SVM\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "clf_svm = SVC(random_state=1,kernel='linear')\n",
    "clf_svm.fit(X_train, y_train)\n",
    "\n",
    "y_predict = clf_svm.predict(X_test)\n",
    "print(sum(y_predict == y_test))\n",
    "print('Accuracy Score is {:.5}'.format(accuracy_score(y_test, y_predict)))\n",
    "print('Recall Score is {:.5}'.format(recall_score(y_test, y_predict)))\n",
    "print('Precision Score is {:.5}'.format(precision_score(y_test, y_predict)))\n",
    "print('F1 Score is {:.5}'.format(f1_score(y_test, y_predict)))\n",
    "\n",
    "sns.set_style('white') \n",
    "class_names = ['0','1']\n",
    "\n",
    "print(pd.DataFrame(confusion_matrix(y_test,y_predict))) # left is actual, top is predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE (Data Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of target variable is imbalanced\n",
    "print(df_card_encoded_scaled['default.payment.next.month'].value_counts())\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE(random_state=1)\n",
    "np.random.seed(1)\n",
    "\n",
    "X_train_balanced, y_train_balanced = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "# Export to csv\n",
    "smoteTrain = X_train_balanced.reset_index(drop=True).join(y_train_balanced)\n",
    "testSet = X_test.join(y_test)\n",
    "\n",
    "# generate csv files for train and test set each\n",
    "smoteTrain.to_csv('~/Downloads/smoteTrain.csv', index=False)\n",
    "testSet.to_csv('~/Downloads/testSet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((X_train_balanced.MARRIED == 1) & (X_train_balanced.SINGLE == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXCe2hUQV8zD"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('smoteTrain.csv')\n",
    "df_test = pd.read_csv('testSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1668591063991,
     "user": {
      "displayName": "yq huang",
      "userId": "12028814288211570124"
     },
     "user_tz": -480
    },
    "id": "f43dCHbqdDrA",
    "outputId": "febc5132-f354-4a13-edc1-461118aa15da"
   },
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaFlT7pbXZcp"
   },
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:,0:-1] # all the variables\n",
    "y_train = df_train.iloc[:,-1] # labels\n",
    "\n",
    "X_test = df_test.iloc[:,0:-1] # all the variables\n",
    "y_test = df_test.iloc[:,-1] # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1668591063992,
     "user": {
      "displayName": "yq huang",
      "userId": "12028814288211570124"
     },
     "user_tz": -480
    },
    "id": "PwXoUr07HZGl",
    "outputId": "363e852a-b9bf-49d9-f671-35b0f9b69c58"
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Selection and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZWIvZznHdFM"
   },
   "source": [
    "### Model 1: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 1043553,
     "status": "error",
     "timestamp": 1668592823927,
     "user": {
      "displayName": "yq huang",
      "userId": "12028814288211570124"
     },
     "user_tz": -480
    },
    "id": "EnNwzwgRHXgV",
    "outputId": "03318e24-ee16-489b-f40d-e7eb48b80cf7"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "model = SVC(random_state=1,C=10,kernel='rbf')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_predict = model.predict(X_test)\n",
    "print(classification_report(y_test, y_predict))\n",
    "print(pd.DataFrame(confusion_matrix(y_test,y_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "executionInfo": {
     "elapsed": 1211,
     "status": "error",
     "timestamp": 1668598998065,
     "user": {
      "displayName": "yq huang",
      "userId": "12028814288211570124"
     },
     "user_tz": -480
    },
    "id": "Ddmxzkmeull9",
    "outputId": "27a2419c-e93c-4848-b3ff-38d2a0605413"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_space = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "clf = GridSearchCV(model, param_space, n_jobs=-1, cv=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters found:\\n', clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 108917,
     "status": "error",
     "timestamp": 1668599149013,
     "user": {
      "displayName": "yq huang",
      "userId": "12028814288211570124"
     },
     "user_tz": -480
    },
    "id": "LV3zFzUGrGmk",
    "outputId": "e561eb5c-3020-42f5-b171-abb3fe1914ac"
   },
   "outputs": [],
   "source": [
    "clf_svm = SVC(random_state=1,kernel='linear')\n",
    "clf_svm.fit(X_train, y_train)\n",
    "y_predict = clf_svm.predict(X_test)\n",
    "print(classification_report(y_test, y_predict))\n",
    "print(pd.DataFrame(confusion_matrix(y_test,y_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVM avg class accuracy (before tuning): \", (1/2 * (0.82 + 0.57)))\n",
    "print(\"SVM avg class accuracy (after tuning): \", (1/2 * (0.88 + 0.51)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#instantiate the model\n",
    "log_regression = LogisticRegression(random_state=5)\n",
    "#fit the model using the training data\n",
    "log_regression.fit(X_train,y_train)\n",
    "#use model to make predictions on test data\n",
    "y_predict = log_regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report for Logistic Regression\")\n",
    "print(classification_report(y_test, y_predict))\n",
    "\n",
    "# Accuracy score\n",
    "print(\"Confusion Matrix for Logistic Regression\")\n",
    "print(pd.DataFrame(confusion_matrix(y_test,y_predict)))\n",
    "\n",
    "# Calcuate accuracy\n",
    "print(\"Accuracy for Logistic Regression: \", accuracy_score(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "TP_LR = 4933 # top left\n",
    "FP_LR = 724 # bottom left\n",
    "TN_LR = 941\n",
    "FN_LR = 902\n",
    "\n",
    "specificity_LR = TN_LR / (TN_LR + FP_LR)\n",
    "print(\"Specificity: \", specificity_LR )\n",
    "print(\"FPR (False Positive Rate): \", 1 - specificity_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curve and find AUC\n",
    "y_pred_proba = log_regression.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "\n",
    "random_probs = [0 for _ in range(len(y_test))]\n",
    "random_fpr, random_tpr, _ = metrics.roc_curve(y_test, random_probs)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "random_auc = metrics.roc_auc_score(y_test, random_probs)\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr,label=\"LogReg AUC=\"+str(auc))\n",
    "plt.plot(random_fpr, random_tpr, label=\"Random AUC=\"+str(random_auc))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning using GridSearchCV\n",
    "# example of grid searching key hyperparametres for logistic regression\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define models and parameters\n",
    "model = LogisticRegression()\n",
    "solvers = [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_train,y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Logistic Regression after hyperparameter tuning\n",
    "#instantiate the model\n",
    "log_regression = LogisticRegression(C = 100, penalty = 'l2', solver = 'newton-cg', random_state=5)\n",
    "#fit the model using the training data\n",
    "log_regression.fit(X_train,y_train)\n",
    "#use model to make predictions on test data\n",
    "y_predict = log_regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report for Logistic Regression\")\n",
    "print(classification_report(y_test, y_predict))\n",
    "\n",
    "# Accuracy score\n",
    "print(\"Confusion Matrix for Logistic Regression\")\n",
    "print(pd.DataFrame(confusion_matrix(y_test,y_predict)))\n",
    "\n",
    "# Calcuate accuracy\n",
    "print(\"Accuracy for Logistic Regression: \", accuracy_score(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "TP_LR = 4925 # top left\n",
    "FP_LR = 721 # bottom left\n",
    "TN_LR = 944\n",
    "FN_LR = 910\n",
    "\n",
    "precision_LR = TP_LR / (TP_LR + FP_LR)\n",
    "specificity_LR = TN_LR / (TN_LR + FP_LR)\n",
    "print(\"Precision: \", precision_LR) \n",
    "print(\"Specificity: \", specificity_LR )\n",
    "print(\"FPR (False Positive Rate): \", 1 - specificity_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curve and find AUC\n",
    "y_pred_proba = log_regression.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "\n",
    "random_probs = [0 for _ in range(len(y_test))]\n",
    "random_fpr, random_tpr, _ = metrics.roc_curve(y_test, random_probs)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "random_auc = metrics.roc_auc_score(y_test, random_probs)\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr,label=\"LogReg AUC=\"+str(auc))\n",
    "plt.plot(random_fpr, random_tpr, label=\"Random AUC=\"+str(random_auc))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic regression avg class accuracy (before tuning): \", (1/2 * (0.85 + 0.57)))\n",
    "print(\"Logistic regression avg class accuracy (after tuning): \", (1/2 * (0.84 + 0.57)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Multi-layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MLPClassifer \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Import accuracy score \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create model object\n",
    "clf = MLPClassifier(hidden_layer_sizes=(6,5),\n",
    "                    random_state=5,\n",
    "                    verbose=True,\n",
    "                    learning_rate_init=0.01)\n",
    "\n",
    "# Fit data onto the model\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Make prediction on test dataset\n",
    "y_predict = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report for MLP\")\n",
    "print(classification_report(y_test, y_predict))\n",
    "\n",
    "print(\"Confusion Matrix for MLP\")\n",
    "print(pd.DataFrame(confusion_matrix(y_test,y_predict)))\n",
    "\n",
    "# Calcuate accuracy\n",
    "print(\"Accuracy for MLP: \", accuracy_score(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "TP_MLP = 4744\n",
    "FP_MLP = 676\n",
    "TN_MLP = 989\n",
    "FN_MLP = 1091\n",
    "\n",
    "specificity_MLP = TN_MLP / (TN_MLP + FP_MLP)\n",
    "print(\"Specificity: \", specificity_MLP )\n",
    "print(\"FPR (False Positive Rate): \", 1 - specificity_MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter tuning for MLP\n",
    "mlp_gs = MLPClassifier(max_iter=100,random_state=5)\n",
    "\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(10,50), (10,50,100), (50, 100, 150), (20,20)],\n",
    "    'activation': ['tanh', 'relu','logistic'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP after parameter tuning\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Import accuracy score \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create model object\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50, 100, 150),\n",
    "                    activation = 'relu', \n",
    "                    alpha = 0.0001,\n",
    "                    random_state=5,\n",
    "                    verbose=True,\n",
    "                    learning_rate = 'constant', \n",
    "                    solver = 'sgd'\n",
    "                    )\n",
    "\n",
    "# Fit data onto the model\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Make prediction on test dataset\n",
    "y_predict = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report for MLP\")\n",
    "print(classification_report(y_test, y_predict))\n",
    "\n",
    "print(\"Confusion Matrix for MLP\")\n",
    "print(pd.DataFrame(confusion_matrix(y_test,y_predict)))\n",
    "\n",
    "# Calcuate accuracy\n",
    "print(\"Accuracy for MLP: \", accuracy_score(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP avg class accuracy (before tuning): \", (1/2 * (0.81 + 0.59)))\n",
    "print(\"MLP avg class accuracy (after tuning): \", (1/2 * (0.83 + 0.57)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: k-nearest neighbors (k-NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "acc = []\n",
    "# Will take some time\n",
    "from sklearn import metrics\n",
    "for i in range(1,200):\n",
    "    neigh = KNeighborsClassifier(n_neighbors = i).fit(X_train,y_train)\n",
    "    yhat = neigh.predict(X_test)\n",
    "    acc.append(metrics.precision_score(y_test, yhat))\n",
    "    \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,200),acc,color = 'blue',linestyle='dashed', \n",
    "         marker='o',markerfacecolor='red', markersize=10)\n",
    "plt.title('precision vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Precision')\n",
    "print(\"Maximum precision: \",max(acc),\" at K = \",acc.index(max(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "# Will take some time\n",
    "from sklearn import metrics\n",
    "for i in range(1,200):\n",
    "    neigh = KNeighborsClassifier(n_neighbors = i).fit(X_train,y_train)\n",
    "    yhat = neigh.predict(X_test)\n",
    "    acc.append(metrics.accuracy_score(y_test, yhat))\n",
    "    \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,200),acc,color = 'blue',linestyle='dashed', \n",
    "         marker='o',markerfacecolor='red', markersize=10)\n",
    "plt.title('accuracy vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Accuracy')\n",
    "print(\"Maximum accuracy: \",max(acc),\" at K = \",acc.index(max(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=180) \n",
    "classifier.fit(X_train, y_train)\n",
    "y_predict = classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_predict))\n",
    "print(pd.DataFrame(confusion_matrix(y_test,y_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"k-NN avg class accuracy: \", (1/2 * (0.78 + 0.61)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Decision Tree and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree and random forest\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,roc_auc_score,roc_curve\n",
    "\n",
    "# Build the decision tree model wuth default parameters\n",
    "dt = DecisionTreeClassifier(random_state=42,max_depth= 5)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable for the testing set\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Get the probability predictions for AUC\n",
    "y_prob = dt.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1-score:', f1)\n",
    "print('Confusion Matrix:\\n', cm)\n",
    "# Calculate the AUC\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"AUC:\", auc)\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Print AUC\n",
    "print('AUC:', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tune parameters\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the range of values to test for the parameters\n",
    "param_grid = {'max_depth': range(1, 10),\n",
    "              'min_samples_split': range(1,10),\n",
    "              'min_samples_leaf': range(1,10)}\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Use GridSearchCV to search for the best combination of parameters\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding mean cross-validation score\n",
    "print('Best parameters:', grid_search.best_params_)\n",
    "print('Best cross-validation score:', grid_search.best_score_)\n",
    "\n",
    "\n",
    "#build the decision tree with tuned parameters\n",
    "dt2 = DecisionTreeClassifier(random_state=42,max_depth= 9, min_samples_leaf=1, min_samples_split= 4)\n",
    "dt2.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = dt2.predict(X_test)\n",
    "\n",
    "# Get the probability predictions for AUC\n",
    "y_prob = dt2.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1-score:', f1)\n",
    "print('Confusion Matrix:\\n', cm)\n",
    "# Calculate the AUC\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"AUC:\", auc)\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Print AUC\n",
    "print('AUC:', auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Tree avg class accuracy (before tuning): \", (1/2 * (0.78 + 0.61)))\n",
    "print(\"Decision Tree avg class accuracy (after tuning): \", (1/2 * (0.74 + 0.61)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a random forest classifier with default hyperparameters\n",
    "rf_best = RandomForestClassifier()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = rf_best.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the random forest classifier on the test set\n",
    "accuracy = rf_best.score(X_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Predict the class probabilities for the test set\n",
    "y_prob = rf_best.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the recall score\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 score:\", f1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Print AUC\n",
    "print('AUC:', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature importances with corresponding feature names\n",
    "importances = rf_best.feature_importances_\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# create a dataframe to store the feature importances\n",
    "df_importances = pd.DataFrame({'feature_names': feature_names, 'importance': importances})\n",
    "\n",
    "# sort the features by importance\n",
    "df_importances = df_importances.sort_values('importance', ascending=False)\n",
    "\n",
    "# plot the feature importances\n",
    "plt.bar(x='feature_names', height='importance', data=df_importances)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Gini Importance of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the model's performance was not outstanding, we can draw insights from Gini importance. \"Gini importance” is the total decrease in node impurity (weighted by the probability of reaching that node (which is approximated by the proportion of samples reaching that node)) averaged over all trees of the ensemble.  \n",
    "\n",
    "Reference: \n",
    "Gordon, A. D., Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification and Regression Trees. Biometrics, 40(3), 874. https://doi.org/10.2307/2530946\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50,100,150],\n",
    "    'max_depth': [5, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "}\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Train the random forest classifier with the best hyperparameters\n",
    "rf_best = RandomForestClassifier(**grid_search.best_params_, random_state=42)\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = rf_best.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate the performance of the random forest classifier on the test set\n",
    "accuracy = rf_best.score(X_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Predict the class probabilities for the test set\n",
    "y_prob = rf_best.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the recall score\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 score:\", f1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Print AUC\n",
    "print('AUC:', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest avg class accuracy (before tuning): \", (1/2 * (0.87 + 0.47)))\n",
    "print(\"Random Forest avg class accuracy (after tuning): \", (1/2 * (0.83 + 0.57)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "f2e492d308b5dc6bc688835395faceb350f33e945e243116376bf9861684b5a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
